{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tic-Tac-Toe environment\n",
    "\n",
    "The [Tic-Tac-Toe](https://github.com/MauroLuzzatto/OpenAI-Gym-TicTacToe-Environment) is a simple game environment that allows to train reinforcement learning agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://img.poki.com/cdn-cgi/image/quality=78,width=600,height=600,fit=cover,f=auto/85535e05d1f130b16751c8308cfbb19b.png\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\n",
    "    url=\"https://img.poki.com/cdn-cgi/image/quality=78,width=600,height=600,fit=cover,f=auto/85535e05d1f130b16751c8308cfbb19b.png\",\n",
    "    width=300,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the python modules\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym_TicTacToe\n",
    "\n",
    "from src.qagent import Qagent\n",
    "from src.player import Player\n",
    "from src.play_tictactoe import play_tictactoe\n",
    "\n",
    "from utils import (\n",
    "    create_state_dictionary,\n",
    "    load_qtable,\n",
    "    reshape_state,\n",
    "    save_qtable,\n",
    ")\n",
    "\n",
    "# ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tictactoe environment\n",
    "env = gym.envs.make(\"TTT-v0\", small=-1, large=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 0, 0, 1, 7, 6, 3, 0, 2, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get 10 randomly sampled actions\n",
    "[env.action_space.sample() for ii in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══╤═══╤═══╕\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]] -1 False\n",
      "╒═══╤═══╤═══╕\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n"
     ]
    }
   ],
   "source": [
    "color = 1\n",
    "action = 4\n",
    "\n",
    "new_state, reward, done, _ = env.step((action, color))\n",
    "print(new_state, reward, done)\n",
    "print(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of legal states: 8953\n"
     ]
    }
   ],
   "source": [
    "state_dict = create_state_dictionary()\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "episodes = 90_000  # 10**6 * 2\n",
    "max_steps = 9\n",
    "\n",
    "# name of the qtable when saved\n",
    "load = False\n",
    "save = True\n",
    "test = True\n",
    "\n",
    "num_test_games = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_parameters = {\"learning_rate\": 1.0, \"gamma\": 0.9}\n",
    "exploration_parameters = {\n",
    "    \"max_epsilon\": 1.0,\n",
    "    \"min_epsilon\": 0.0,\n",
    "    \"decay_rate\": 0.000005,\n",
    "}\n",
    "\n",
    "name = f\"qtable_{episodes}\"\n",
    "folder = \"tables\"\n",
    "\n",
    "qagent = Qagent(state_size, action_size, learning_parameters, exploration_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(player, state, action_space):\n",
    "\n",
    "    action = qagent.get_action(state, action_space)\n",
    "\n",
    "    # remove action from the action space\n",
    "    action_space = action_space[action_space != action]\n",
    "\n",
    "    new_state, reward, done, _ = env.step((action, player.color))\n",
    "    new_state = state_dict[reshape_state(new_state)]\n",
    "\n",
    "    qagent.qtable[state, action] = qagent.update_qtable(\n",
    "        state, new_state, action, reward, done\n",
    "    )\n",
    "    # new state\n",
    "    state = new_state\n",
    "    player.add_reward(reward)\n",
    "    return state, action_space, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 36/90000 [00:00<04:14, 352.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0,             epsilon: 1.0,             sum q-table: 3.0,             elapsed time [min]: 0.0,              done [%]: 0.0             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10068/90000 [00:28<03:23, 393.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10000,             epsilon: 0.95,             sum q-table: 95966.98500999999,             elapsed time [min]: 0.47,              done [%]: 11.11111111111111             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20048/90000 [00:54<02:48, 414.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 20000,             epsilon: 0.9,             sum q-table: 136012.59715,             elapsed time [min]: 0.91,              done [%]: 22.22222222222222             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 30070/90000 [01:20<02:11, 456.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 30000,             epsilon: 0.86,             sum q-table: 147013.89811,             elapsed time [min]: 1.34,              done [%]: 33.33333333333333             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 40055/90000 [01:47<02:38, 314.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 40000,             epsilon: 0.82,             sum q-table: 151038.5526,             elapsed time [min]: 1.8,              done [%]: 44.44444444444444             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 50076/90000 [02:15<01:33, 428.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 50000,             epsilon: 0.78,             sum q-table: 152085.8446,             elapsed time [min]: 2.26,              done [%]: 55.55555555555556             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 60060/90000 [02:44<01:26, 346.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 60000,             epsilon: 0.74,             sum q-table: 153296.5445,             elapsed time [min]: 2.75,              done [%]: 66.66666666666666             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 70025/90000 [03:12<01:39, 200.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 70000,             epsilon: 0.7,             sum q-table: 153697.5919,             elapsed time [min]: 3.21,              done [%]: 77.77777777777779             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 80049/90000 [03:41<00:27, 361.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 80000,             epsilon: 0.67,             sum q-table: 153990.49790000002,             elapsed time [min]: 3.69,              done [%]: 88.88888888888889             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90000/90000 [04:07<00:00, 363.26it/s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "player_1 = Player(color=1, episodes=episodes)\n",
    "player_2 = Player(color=2, episodes=episodes)\n",
    "\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    state = env.reset()\n",
    "    state = state_dict[reshape_state(state)]\n",
    "\n",
    "    action_space = np.arange(9)\n",
    "\n",
    "    player_1.reset_reward()\n",
    "    player_2.reset_reward()\n",
    "\n",
    "    # change start of players, randomly change the order players\n",
    "    # to start the game, integer either 0 or 1\n",
    "    start = np.random.randint(2)\n",
    "\n",
    "    for _step in range(start, max_steps + start):\n",
    "\n",
    "        # alternate the moves of the players\n",
    "        if _step % 2 == 0:\n",
    "            state, action_space, done = play(player_1, state, action_space)\n",
    "        else:\n",
    "            state, action_space, done = play(player_2, state, action_space)\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # reduce epsilon for exporation-exploitation tradeoff\n",
    "    qagent.update_epsilon(episode)\n",
    "    player_1.save_reward(episode)\n",
    "    player_2.save_reward(episode)\n",
    "\n",
    "    if episode % 1_0000 == 0:\n",
    "\n",
    "        sum_q_table = np.sum(qagent.qtable)\n",
    "        time_passed = round((time.time() - start_time) / 60.0, 2)\n",
    "\n",
    "        print(\n",
    "            f\"episode: {episode}, \\\n",
    "            epsilon: {round(qagent.epsilon, 2)}, \\\n",
    "            sum q-table: {sum_q_table}, \\\n",
    "            elapsed time [min]: {time_passed},  \\\n",
    "            done [%]: {episode / episodes * 100} \\\n",
    "            \"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtable_90000.npy saved!\n"
     ]
    }
   ],
   "source": [
    "qtable = qagent.get_qtable()\n",
    "\n",
    "if save:\n",
    "    save_qtable(qtable, folder, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human beginns\n",
      "--------------------\n",
      "╒═══╤═══╤═══╕\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 5\n",
      "-1\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 0\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ X │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 4\n",
      "-1\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 1\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ X │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 3\n",
      "9\n",
      "********************\n",
      "Human won!\n",
      "********************\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ X │ X │ X │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "\n",
      "\n",
      "\n",
      "Agent beginns\n",
      "--------------------\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 0\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 4\n",
      "-1\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 1\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 5\n",
      "-1\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 6\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ X │\n",
      "├───┼───┼───┤\n",
      "│ O │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 3\n",
      "9\n",
      "********************\n",
      "Human won!\n",
      "********************\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ X │ X │ X │\n",
      "├───┼───┼───┤\n",
      "│ O │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "\n",
      "\n",
      "\n",
      "Human beginns\n",
      "--------------------\n",
      "╒═══╤═══╤═══╕\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 4\n",
      "-1\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 0\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 8\n",
      "-1\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 5\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ O │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ X │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 2\n",
      "-1\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 1\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ O │ X │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ O │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ X │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maurol\\OneDrive\\Dokumente\\Python_Scripts\\learn-tictactoe-through-self-play\\notebook_learn_tictactoe.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/notebook_learn_tictactoe.ipynb#ch0000012?line=0'>1</a>\u001b[0m \u001b[39m# test the algorithm with playing against it\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/notebook_learn_tictactoe.ipynb#ch0000012?line=1'>2</a>\u001b[0m play_tictactoe(env, qagent\u001b[39m.\u001b[39;49mqtable, max_steps, state_dict)\n",
      "File \u001b[1;32mc:\\Users\\maurol\\OneDrive\\Dokumente\\Python_Scripts\\learn-tictactoe-through-self-play\\utils.py:119\u001b[0m, in \u001b[0;36mplay_tictactoe\u001b[1;34m(env, qtable, max_steps, state_dict, num_test_games)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/utils.py?line=115'>116</a>\u001b[0m action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mnan\n\u001b[0;32m    <a href='file:///c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/utils.py?line=117'>118</a>\u001b[0m \u001b[39mwhile\u001b[39;00m action \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m action_space:\n\u001b[1;32m--> <a href='file:///c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/utils.py?line=118'>119</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(\n\u001b[0;32m    <a href='file:///c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/utils.py?line=119'>120</a>\u001b[0m         \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mchoose an action from \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m: \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(action_space))\n\u001b[0;32m    <a href='file:///c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/utils.py?line=120'>121</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/utils.py?line=121'>122</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAction:\u001b[39m\u001b[39m\"\u001b[39m, action)\n\u001b[0;32m    <a href='file:///c%3A/Users/maurol/OneDrive/Dokumente/Python_Scripts/learn-tictactoe-through-self-play/utils.py?line=122'>123</a>\u001b[0m action_space \u001b[39m=\u001b[39m action_space[action_space \u001b[39m!=\u001b[39m action]\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "# test the algorithm with playing against it\n",
    "play_tictactoe(env, qtable, max_steps, state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6050d35557e2eda2bee3489ac5b9239cf3ea28e67ca6bb3b65a2efaf99506245"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tictactoe_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
